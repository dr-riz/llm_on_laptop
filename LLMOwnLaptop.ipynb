{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be2c3d7-e637-4ca0-b823-1442ce1ef014",
   "metadata": {},
   "source": [
    "# Running an LLM on your own laptop\n",
    "In this notebook, we're going to learn how to run a Hugging Face LLM on our own machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c88379d-de11-4f6b-b3b2-dd1c86ec1f2d",
   "metadata": {},
   "source": [
    "## Download the LLM\n",
    "We're going to write some code to manually download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eba955c-10f9-435a-8f5c-a4f3148214f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1769883-5d03-45a6-89a4-53d97a2b31cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_API_KEY = os.environ.get(\"HUGGING_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d4a3cf-3aef-48a2-9d9c-c0e6f47b8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "filenames = [\n",
    "        \"pytorch_model.bin\", \"added_tokens.json\", \"config.json\", \"generation_config.json\", \n",
    "        \"special_tokens_map.json\", \"spiece.model\", \"tokenizer_config.json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de36f22-0e89-4add-9c2b-bbb34c4d9d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rmian/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024/pytorch_model.bin\n",
      "/Users/rmian/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024/added_tokens.json\n",
      "/Users/rmian/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024/config.json\n",
      "/Users/rmian/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024/generation_config.json\n",
      "/Users/rmian/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024/special_tokens_map.json\n",
      "/Users/rmian/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024/spiece.model\n",
      "/Users/rmian/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "for filename in filenames:\n",
    "        downloaded_model_path = hf_hub_download(\n",
    "                    repo_id=model_id,\n",
    "                    filename=filename,\n",
    "                    token=HUGGING_FACE_API_KEY\n",
    "        )\n",
    "        print(downloaded_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4ad22-2e7d-40d9-978b-b730bd06b141",
   "metadata": {},
   "source": [
    "## Run the LLM\n",
    "Now let's try running the model. But before we do that, let's disable the Wi-Fi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "350ebdd8-f694-4453-a991-3ad62fd9f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the current directory is in the Python path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba850f47-09e6-4b3d-9042-e4df427b0f09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False, use_fast=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False, use_fast=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "pipeline = pipeline(\"text2text-generation\", model=model, device=-1, tokenizer=tokenizer, max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93138286-8497-4730-9da4-3f52390bd380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Apache Kafka is a popular open source message broker that is used for real-time data streaming and streaming applications. It is a popular choice for companies that need to process large amounts of data quickly and efficiently. Some of the competitors to Apache Kafka include Apache Spark, Apache Kafka Streams, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Streaming, Apache Kafka Streams Stream'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"What are competitors to Apache Kafka?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca79ce5d-2e44-446c-a899-d9ecc4b77caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"No,   I   don't   have   a   sister. \\n\"}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"\"\"My name is Mark.\n",
    "I have brothers called David and John and my best friend is Michael.\n",
    "Using only the context above. Do you know if I have a sister?    \n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
